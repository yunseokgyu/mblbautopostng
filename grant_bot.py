import wp_utils
import requests
import xml.etree.ElementTree as ET
import datetime
import os
import json
import re
from bs4 import BeautifulSoup
from utils.grant_ai import analyze_grant_as_expert
# from bot_status import update_status # Removed invalid import
# Since bot_status.json is shared, let's redefine update_status here locally to avoid circular imports or just import if available. 
# Actually stock_bot.py had it locally. Let's make a shared util later. For now, local is fine.

STATUS_FILE = "bot_status_grant.json"

def update_status(state, message, progress=0.0):
    data = {
        "state": state,
        "message": message,
        "progress": progress,
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    try:
        with open(STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[ERROR] Status save failed: {e}")

def fetch_google_news_rss(query):
    """
    Google News RSSë¥¼ í†µí•´ ê´€ë ¨ í‚¤ì›Œë“œì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    (API Key í•„ìš” ì—†ìŒ, Real Data)
    """
    base_url = "https://news.google.com/rss/search"
    params = {
        "q": query,
        "hl": "ko",
        "gl": "KR",
        "ceid": "KR:ko"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"[ERROR] RSS Fetch failed: {e}")
        return None

def parse_rss_items(xml_content, limit=3):
    """
    XMLì„ íŒŒì‹±í•˜ì—¬ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    items = []
    try:
        root = ET.fromstring(xml_content)
        # channel -> item
        for item in root.findall('.//item')[:limit]:
            title = item.find('title').text if item.find('title') is not None else "No Title"
            link = item.find('link').text if item.find('link') is not None else ""
            description = item.find('description').text if item.find('description') is not None else ""
            pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
            
            # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•˜ê²Œ)
            # descriptionì— HTMLì´ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìŒ.
            
            items.append({
                "title": title,
                "link": link,
                "description": description,
                "pub_date": pub_date
            })
    except Exception as e:
        print(f"[ERROR] XML Parsing failed: {e}")
        
    return items


    return items

def fetch_exportvoucher_announcements(limit=5):
    """
    ìˆ˜ì¶œë°”ìš°ì²˜ ì‚¬ì—…ê³µê³  í¬ë¡¤ë§
    URL: https://www.exportvoucher.com/portal/board/boardList?bbs_id=1
    """
    url = "https://www.exportvoucher.com/portal/board/boardList?bbs_id=1"
    items = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ëª©ë¡ í…Œì´ë¸”
        rows = soup.select('tbody tr')
        if not rows:
            rows = soup.select('tr') # Fallback

        for row in rows[:limit]:
            # ì œëª©
            title_tag = row.select_one('td.left a')
            if not title_tag: 
                # Fallback
                 links = row.find_all('a')
                 for l in links:
                     if 'goDetail' in l.get('onclick', ''):
                         title_tag = l
                         break
            
            if not title_tag: continue
            
            title = title_tag.get_text(strip=True)
            
            # ë§í¬ (onclick="goDetail('3524')")
            onclick = title_tag.get('onclick', '')
            link = ""
            match = re.search(r"goDetail\(['\"]?(\d+)['\"]?\)", onclick)
            if match:
                id_code = match.group(1)
                link = f"https://www.exportvoucher.com/portal/board/boardView?bbs_id=1&ntt_id={id_code}"
            
            # ë‚ ì§œ (3ë²ˆì§¸ td)
            tds = row.find_all('td')
            date_text = tds[2].get_text(strip=True) if len(tds) > 2 else ""
            
            items.append({
                "title": title,
                "link": link,
                "description": title,
                "pub_date": date_text,
                "source_tag": "ìˆ˜ì¶œë°”ìš°ì²˜"
            })
    except Exception as e:
        print(f"[ERROR] ExportVoucher Crawling failed: {e}")
    return items

def fetch_manufacturing_mssd(limit=5):
    """
    ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€(ì œì¡°í˜ì‹ ë°”ìš°ì²˜ ë“±) RSS íŒŒì‹±
    URL: https://mss.go.kr/rss/smba/board/90.do
    """
    rss_url = "https://mss.go.kr/rss/smba/board/90.do"
    items = []
    try:
        import feedparser
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # requestsë¡œ ë¨¼ì € ê°€ì ¸ì˜´ (ì°¨ë‹¨ ë°©ì§€)
        resp = requests.get(rss_url, headers=headers, timeout=10)
        
        feed = feedparser.parse(resp.content)
        
        # Feedparserê°€ ëª» ì°¾ìœ¼ë©´ BS4ë¡œ ì‹œë„
        entries = feed.entries
        if len(entries) == 0:
            soup = BeautifulSoup(resp.content, 'xml')
            xml_items = soup.find_all(['item', 'entry'])
            for x in xml_items[:limit]:
                 t = x.find('title')
                 l = x.find('link')
                 if t:
                     items.append({
                        "title": t.get_text(strip=True),
                        "link": l.get_text(strip=True) if l else "",
                        "description": t.get_text(strip=True),
                        "pub_date": "", 
                        "source_tag": "ì œì¡°ë°”ìš°ì²˜(ì¤‘ê¸°ë¶€)"
                     })
            return items # BS4 ê²°ê³¼ ë°˜í™˜
            
        for entry in entries[:limit]:
            items.append({
                "title": entry.title,
                "link": entry.link,
                "description": entry.description if 'description' in entry else entry.title,
                "pub_date": entry.published if 'published' in entry else "",
                "source_tag": "ì œì¡°ë°”ìš°ì²˜(ì¤‘ê¸°ë¶€)"
            })
    except Exception as e:
        print(f"[ERROR] Manufacturing RSS failed: {e}")
    return items

def fetch_sbiz24_announcements(limit=5):
    """
    ì†Œìƒê³µì¸24 (SPA) í¬ë¡¤ë§ - í˜„ì¬ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì •ì´ í•„ìš”í•¨.
    ì„ì‹œë¡œ requestsë¡œ ë˜ëŠ”ì§€ ì‹œë„í•´ë³´ê³  ì•ˆë˜ë©´ ìŠ¤í‚µ.
    """
    # SPAë¼ì„œ ë‹¨ìˆœ requestsë¡œëŠ” ì•ˆë  í™•ë¥  ë†’ìŒ. 
    # í•˜ì§€ë§Œ ì¼ë‹¨ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ë‘ .
    print("[WARNING] ì†Œìƒê³µì¸24ëŠ” SPA êµ¬ì¡°ë¼ í˜„ì¬ ì§ì ‘ í¬ë¡¤ë§ì´ ì–´ë µìŠµë‹ˆë‹¤. (API ë¶„ì„ í•„ìš”)")
    return []

def fetch_kstartup_announcements(limit=5):
    """
    K-Startup ì‚¬ì—…ê³µê³ (ì§„í–‰ì¤‘) í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    URL: https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do
    """
    url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
    items = []
    
    try:
        # User-Agent ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê³µê³  ë¦¬ìŠ¤íŠ¸ (div#bizPbancList > ul > li)
        list_container = soup.find('div', id='bizPbancList')
        if not list_container:
            print("[WARNING] K-Startup ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ(div)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ classë¡œë„ ì‹œë„
            list_container = soup.find('ul', class_='board_list')
            
        if not list_container:
             # ìµœí›„ì˜ ìˆ˜ë‹¨: ì „ì²´ì—ì„œ li.notice ì°¾ê¸°
             li_list = soup.find_all('li', class_='notice')
        else:
             li_list = list_container.find_all('li', class_='notice')
        
        for li in li_list[:limit]:
            # 1. ì œëª© & ë§í¬ ID ì¶”ì¶œ
            title_tag = li.find('p', class_='tit')
            if not title_tag: 
                title_tag = li.find('a') # pê°€ ì—†ìœ¼ë©´ aíƒœê·¸ ìì²´ê°€ ì œëª©ì¼ ìˆ˜ ìˆìŒ
                
            title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
            
            # ë§í¬ (javascript:go_view(123456))
            link_tag = li.find('a', href=True)
            link = ""
            if link_tag:
                href = link_tag['href']
                # go_view( ìˆ«ì ) ì¶”ì¶œ
                match = re.search(r"go_view\('?(\d+)'?\)", href)
                if match:
                    id_code = match.group(1)
                    # ìƒì„¸ í˜ì´ì§€ URL ì¡°í•©
                    # pbancClssCdëŠ” ë³´í†µ ì¤‘ì•™ë¶€ì²˜(PBC010)ì´ë‚˜, ì—†ì–´ë„ view ëª¨ë“œì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”.
                    # ì•ˆì „í•˜ê²Œ paramì— ë„£ì–´ì„œ ì´ë™
                    link = f"https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={id_code}"
            
            # 2. ë‚ ì§œ ì¶”ì¶œ (span.list ì•ˆì— "ë“±ë¡ì¼ì yyyy-mm-dd")
            date_text = ""
            bottom_div = li.find('div', class_='bottom')
            if bottom_div:
                spans = bottom_div.find_all('span', class_='list')
                for sp in spans:
                    txt = sp.get_text(strip=True)
                    if "ë“±ë¡ì¼ì" in txt:
                        # "ë“±ë¡ì¼ì 2024-01-01" -> "2024-01-01"
                        date_text = txt.replace("ë“±ë¡ì¼ì", "").strip()
                        break
            
            # ì„¤ëª…ì€ ëª©ë¡ì— ì—†ìœ¼ë¯€ë¡œ ì œëª©ê³¼ ë™ì¼í•˜ê²Œ or ë¹ˆê°’
            
            items.append({
                "title": title,
                "link": link,
                "description": title, # K-Startup ëª©ë¡ì—ëŠ” ë³¸ë¬¸ ìš”ì•½ì´ ì—†ìŒ
                "pub_date": date_text,
                "source_tag": "K-Startup"
            })
            
    except Exception as e:
        print(f"[ERROR] K-Startup Crawling failed: {e}")
        
    return items

def run_grant_job(dry_run=True, limit=None):
    """
    dry_run=True: í¬ìŠ¤íŒ…ì€ í•˜ì§€ ì•Šê³  ìˆ˜ì§‘/ë¶„ì„ë§Œ ìˆ˜í–‰ (ë¡œê·¸ í™•ì¸ìš©)
    limit: ì²˜ë¦¬í•  ìµœëŒ€ ê³µê³  ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    print(f"[INFO] [Grant Bot] Started. (Dry Run: {dry_run}, Limit: {limit})")
    update_status("running", "[START] ì§€ì›ì‚¬ì—… ê³µê³  ìˆ˜ì§‘ ì‹œì‘...", 0.1)
    
    config = load_config()
    grant_config = config.get('grant', {})
    categories = grant_config.get('categories', {})
    sources = grant_config.get('sources', [])
    
    total_published = 0
    total_found = 0
    
    # Merge items significantly or process one by one
    # Let's collect all items first to deduplicate based on link/title
    all_items = []
    
    # 0. íŠ¹ìˆ˜ í¬ë¡¤ëŸ¬ (Direct Crawling)
    crawlers_config = grant_config.get('crawlers', {'kstartup': True, 'export': True, 'mssd': True, 'sbiz': True})
    
    # K-Startup
    if crawlers_config.get('kstartup', True):
        update_status("running", "[CRAWL] K-Startup ê³µê³  ìˆ˜ì§‘ ì¤‘...", 0.15)
        print("\n[SOURCE] K-Startup")
        k_items = fetch_kstartup_announcements(limit=5)
        print(f"   -> {len(k_items)}ê°œ ë°œê²¬")
        all_items.extend(k_items)

    # ìˆ˜ì¶œë°”ìš°ì²˜
    if crawlers_config.get('export', True):
        print("\n[SOURCE] ìˆ˜ì¶œë°”ìš°ì²˜ (ExportVoucher)")
        ex_items = fetch_exportvoucher_announcements(limit=5)
        print(f"   -> {len(ex_items)}ê°œ ë°œê²¬")
        all_items.extend(ex_items)

    # ì œì¡°ë°”ìš°ì²˜ (ì¤‘ê¸°ë¶€ RSS)
    if crawlers_config.get('mssd', True):
        print("\n[SOURCE] ì œì¡°ë°”ìš°ì²˜ (ì¤‘ê¸°ë¶€ RSS)")
        ms_items = fetch_manufacturing_mssd(limit=5)
        print(f"   -> {len(ms_items)}ê°œ ë°œê²¬")
        all_items.extend(ms_items)

    # ì†Œìƒê³µì¸24
    if crawlers_config.get('sbiz', True):
        # sbiz = fetch_sbiz24_announcements() # ì•„ì§ ë¯¸ì™„ì„±
        pass
    
    # 0-2. AI ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘ (ì‚¬ìš©ì ì •ì˜ HTML ì‚¬ì´íŠ¸)
    ai_sources = grant_config.get('ai_sources', [])
    if ai_sources:
        print("\n[SOURCE] AI ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘ (Beta)")
        from utils.grant_ai import extract_announcements_from_html
        
        for ai_url in ai_sources:
            update_status("running", f"[AI] {ai_url} ë¶„ì„ ì¤‘...", 0.3)
            print(f"   -> Analyzing: {ai_url}")
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                resp = requests.get(ai_url, headers=headers, timeout=15)
                if resp.status_code == 200:
                    extracted = extract_announcements_from_html(resp.text, base_url=ai_url)
                    print(f"      Found {len(extracted)} items (by Gemini)")
                    
                    for item in extracted:
                        # AIê°€ ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ key mapping
                        all_items.append({
                            "title": item.get('title'),
                            "link": item.get('link'),
                            "description": f"AI ìˆ˜ì§‘: {item.get('title')}",
                            "pub_date": item.get('date', ''),
                            "source_tag": "AIìˆ˜ì§‘"
                        })
                else:
                    print(f"      [Fail] Status {resp.status_code}")
            except Exception as e:
                print(f"      [Error] {e}")
    
    # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ (Google News) - ì‚¬ìš©ìê°€ ë¹„í™œì„±í™” ìš”ì²­í•¨
    # if categories:
    #     cat_keys = list(categories.keys())
    #     for c_idx, (category_name, keywords) in enumerate(categories.items()):
    #         progress = 0.2 + (c_idx / len(cat_keys)) * 0.3
    #         update_status("running", f"[SEARCH] '{category_name}' ê´€ë ¨ ê³µê³  (Google News)", progress)
    #         
    #         if not keywords: continue
    #         query = " OR ".join(keywords)
    #         
    #         rss_xml = fetch_google_news_rss(query)
    #         if rss_xml:
    #             items = parse_rss_items(rss_xml, limit=3)
    #             for item in items:
    #                 item['source_tag'] = category_name 
    #                 all_items.append(item)

    # 2. ë§ì¶¤ RSS ì†ŒìŠ¤
    if sources:
        update_status("running", "[RSS] ë§ì¶¤ ì†ŒìŠ¤ ìˆ˜ì§‘ ì¤‘...", 0.5)
        for s_idx, source_url in enumerate(sources):
            rss_xml = fetch_custom_rss(source_url)
            if rss_xml:
                items = parse_rss_items(rss_xml, limit=5)
                for item in items:
                    item['source_tag'] = "ë§ì¶¤ê³µê³ "
                    all_items.append(item)
    
    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    unique_items = {item['link']: item for item in all_items}.values()
    print(f"[INFO] ì´ {len(unique_items)}ê°œì˜ ê³µê³  ìˆ˜ì§‘ë¨.")
    
    # WP ìµœê·¼ ê¸€ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ë°©ì§€ìš©)
    recent_posts = wp_utils.get_recent_posts(limit=30)
    
    # ì¹´í…Œê³ ë¦¬ ID í™•ë³´ ('government subsidies')
    cat_id = wp_utils.ensure_category("government subsidies")
    cat_ids = [cat_id] if cat_id else []

    target_items = []
    
    for item in unique_items:
        if limit and len(target_items) >= limit:
            break
            
        title = item['title']
        
        # ì¤‘ë³µ ì²´í¬
        is_duplicate = False
        # 1. WP ì²´í¬
        for p in recent_posts:
            # ì œëª©ì˜ ì¼ë¶€ê°€ ê²¹ì¹˜ê±°ë‚˜ ë§í¬(ë³¸ë¬¸ì— ìˆì„ ìˆ˜ ìˆìŒ) ì²´í¬ëŠ” ì–´ë µì§€ë§Œ ì œëª©ìœ¼ë¡œ 1ì°¨ í•„í„°
            # ë¹„ìŠ·í•˜ë©´ ìŠ¤í‚µ (ê°„ë‹¨ ë§¤ì¹­)
            if title[:len(title)//2] in p['title']: # ì œëª© ì• ì ˆë°˜ì´ ê°™ìœ¼ë©´ ì˜ì‹¬
                is_duplicate = True
                break
        
        if is_duplicate:
            print(f"[SKIP] ì´ë¯¸ ë°œí–‰ëœ ê³µê³ (WP): {title}")
            continue
            
        target_items.append(item)
        
    print(f"[INFO] ë¶„ì„ ëŒ€ìƒ: {len(target_items)}ê°œ")
    update_status("running", f"[ANALYSIS] {len(target_items)}ê°œ ê³µê³  ë¶„ì„ ì‹œì‘...", 0.6)

    count = 0
    total = len(target_items)
    
    for i, item in enumerate(target_items):
        process_grant_item(item, item.get('source_tag', 'ê¸°íƒ€'), dry_run, cat_ids)
        update_status("running", f"[POSTING] {i+1}/{total} ì²˜ë¦¬ ì¤‘...", 0.6 + (i/total)*0.4)

    update_status("idle", f"ì™„ë£Œ. (ìˆ˜ì§‘: {len(all_items)}, ìµœì¢…: {len(target_items)})", 1.0)


def process_grant_item(item, category_tag, dry_run, cat_ids):
    """
    ê³µí†µ ì•„ì´í…œ ì²˜ë¦¬ ë¡œì§ (ë¶„ì„ -> í¬ìŠ¤íŒ…)
    """
    title = item['title']
    link = item['link']
    description = item['description']
    pub_date = item['pub_date']
    
    print(f"\n[Item] {title}")
    
    if dry_run:
        print("   -> [Dry Run] Posting skipped. (Analyzed internally)")
        # Dry Runì´ì–´ë„ ë¶„ì„ í€„ë¦¬í‹° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•œë²ˆ ì°ì–´ë³¼ ìˆ˜ ìˆìŒ
        # analysis = analyze_grant_as_expert(title, description, link)
        # print(analysis[:200] + "...")
        return

    # ì „ë¬¸ê°€ ë¶„ì„
    expert_analysis = analyze_grant_as_expert(title, description, link)
    if "ì˜¤ë¥˜ ë°œìƒ" in expert_analysis:
        print("[SKIP] ë¶„ì„ ì˜¤ë¥˜")
        return

    # ì´ë¯¸ì§€ ì²¨ë¶€ (ë¬´ë£Œ ì´ë¯¸ì§€ 5ê°œ)
    images_html = ""
    try:
        from image_factory import fetch_free_images
        
        # 1. ì œëª©ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
        search_query = title
        # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œì´ ì–´ë µì§€ë§Œ, PexelsëŠ” ê¸´ ì¿¼ë¦¬ë„ ëŒ€ì¶© ì²˜ë¦¬í•¨.
        # ì • ì•ˆë˜ë©´ 'Startup' ê°™ì€ê±¸ë¡œ Fallback
        
        img_urls = fetch_free_images(search_query, count=5)
        if not img_urls:
            print("   -> ì œëª© ê²€ìƒ‰ ì‹¤íŒ¨, 'Startup' í‚¤ì›Œë“œë¡œ ëŒ€ì²´ ê²€ìƒ‰")
            img_urls = fetch_free_images("Startup business team", count=5)
            
        if img_urls:
            print(f"   -> {len(img_urls)}ê°œ ì´ë¯¸ì§€ ì¤€ë¹„ë¨ (Cloudinary Optimized)")
            
            # HTML ìƒì„± (2ì—´ ê·¸ë¦¬ë“œ)
            if img_urls:
                images_html += '<div style="margin-top: 30px;"><h3>ğŸ“· ê´€ë ¨ ì´ë¯¸ì§€</h3>'
                images_html += '<div style="display: flex; flex-wrap: wrap; gap: 10px;">'
                for u in img_urls:
                    images_html += f'<img src="{u}" style="width: 48%; height: auto; object-fit: cover; border-radius: 5px; margin-bottom: 10px;" loading="lazy">'
                images_html += '</div></div>'
                
    except Exception as e:
        print(f"   [Image Attachment Error] {e}")

    # íƒœê·¸ ë¶™ì—¬ì„œ í¬ìŠ¤íŒ…
    wp_title = f"[{category_tag}] {title} - ì „ë¬¸ê°€ ë¶„ì„"
    
    wp_content = f"""
    <p><i>ì´ ê¸€ì€ ì •ë¶€ì§€ì›ê¸ˆ ë°ì´í„°ì™€ AI ì „ë¬¸ê°€ì˜ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.</i></p>
    <div style="background-color: #f6f8fa; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
        <h3>ğŸ“¢ ê³µê³  ìš”ì•½</h3>
        <ul>
            <li><strong>ì œëª©:</strong> {title}</li>
            <li><strong>ì¹´í…Œê³ ë¦¬:</strong> {category_tag}</li>
            <li><strong>ë°œí–‰ì¼:</strong> {pub_date}</li>
        </ul>
        <p style="text-align: center; margin-top: 15px;">
            <a href="{link}" target="_blank" style="background-color: #2ea44f; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;">ğŸ‘‰ ê³µì‹ ê³µê³ ë¬¸ ë³´ëŸ¬ê°€ê¸°</a>
        </p>
    </div>
    <hr>
    {expert_analysis}
    {images_html}
    <hr>
    <p style="color: #666; font-size: 0.9em;">â€» ë³¸ ë¶„ì„ì€ AIì— ì˜í•´ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì •í™•í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê³µì‹ ê¸°ê´€ì˜ ê³µê³ ë¥¼ ì¬í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</p>
    """
    
    res = wp_utils.post_article(wp_title, wp_content, category_ids=cat_ids)
    if res:
        print(f"[SUCCESS] Posted: {wp_title}")
    else:
        print(f"[FAILURE] Failed to post: {wp_title}")

def load_config():
    try:
        with open('bot_config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

if __name__ == "__main__":
    import sys
    import argparse

    parser = argparse.ArgumentParser(description='Grant Bot')
    parser.add_argument('--post', action='store_true', help='Actually post to WordPress')
    parser.add_argument('--limit', type=int, default=None, help='Limit number of posts')
    
    parser.add_argument('--loop', action='store_true', help='Run in infinite loop mode')
    
    args = parser.parse_args()
    
    if args.loop:
        print("[SYSTEM] Grant Bot Starting (Loop Mode)")
        import time
        while True:
            try:
                run_grant_job(dry_run=not args.post, limit=args.limit)
                print("[SYSTEM] Cycle finished. Sleeping for 6 hours...")
                update_status("idle", "[WAIT] ë‹¤ìŒ ì‚¬ì´í´ ëŒ€ê¸° ì¤‘ (6ì‹œê°„)", 1.0)
                time.sleep(6 * 3600)
            except KeyboardInterrupt:
                print("[SYSTEM] Bot stopped by user.")
                break
            except Exception as e:
                print(f"[CRITICAL ERROR] Bot crashed: {e}")
                update_status("error", f"[ERROR] ë´‡ í¬ë˜ì‹œ: {str(e)}", 0.0)
                print("[SYSTEM] Restarting in 60 seconds...")
                time.sleep(60)
    else:
        run_grant_job(dry_run=not args.post, limit=args.limit)
